# -*- coding: utf-8 -*-
"""LTH_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LrHy_lf6rfemQ0enNTwpllPHFA3yAaV1
"""

import os
import sys
import numpy as np
import copy
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torchvision.models as models
import matplotlib.pyplot as plt
from torchsummary import summary
import os
import torchvision.utils as vutils
import seaborn as sns
import torch.nn.init as init
import pickle
from google.colab import drive

# Github repos we went through during implementation
# https://github.com/VITA-Group/PrAC-LTH
# https://github.com/facebookresearch/open_lth/tree/main/lottery
# https://github.com/rahulvigneswaran/Lottery-Ticket-Hypothesis-in-Pytorch

drive.mount("/content/drive", force_remount=True)

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/drive/My Drive/Deep Learning Project/Deliverable 3/"
!ls

path = os.getcwd()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
datatype = 'float32'
print(path)
print(device)
print(datatype)

# Hyperparameters
lr = 0.01
input_shape = (3, 32, 32)
epochs = 20
gatepochs = 5
batch_size = 256
prune_iterations = 10
prune_percent = 20
num_classes = 10
modelType = "vgg11"
datasetName = "cifar10"
procedure = "structuredLTrandom"

"""# Helper functions"""

def checkdir(directory):
  if not os.path.exists(directory):
    os.makedirs(directory)

def model_grad(model, gates, gate=True, conv=True):
  for m in model.modules():
    if isinstance(m, gates):
      for name, param in m.named_parameters():
        if 'weight' in name or "bias" in name:
          param.requires_grad = gate
    else:
      for name, param in m.named_parameters():
        if "weight" in name or "bias" in name:
          param.requires_grad = conv
  model = model.to(device)
  return model

def make_mask(model, gates):
  mask = []
  step = 0
  
  for m in model.modules():
    if isinstance(m, gates):
      step += 1

  mask = [None]*step
  step = 0

  for m in model.modules():
    if isinstance(m, gates):
      for name, param in m.named_parameters():
        mask[step] = np.ones_like(param.data.cpu().numpy()).astype(datatype)
        step +=1
  
  return mask

def prune(model, mask, percent, gates):
  step = 0
  for m in model.modules():
    if isinstance(m, gates):
      for name, param in m.named_parameters():
        if "weight" in name:
          tensor = param.data.cpu().numpy()
          alive = tensor[np.nonzero(tensor)]
          percentile_value = np.percentile(abs(alive), percent)

          weight_dev = param.device
          new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])

          param.data = torch.from_numpy((tensor * new_mask).astype(datatype)).to(weight_dev) 
          # param.data = torch.from_numpy(new_mask.astype(datatype)).to(weight_dev) 
          mask[step] = new_mask
          step += 1
        
  return model, mask 

def lottery_init(model, mask, state_dict, gates):
  step = 0
  model.load_state_dict(state_dict)
  for m in model.modules():
    if isinstance(m, gates):
      for name, param in m.named_parameters():
        if "weight" in name:
          tensor = param.data.cpu().numpy()
          weight_dev = param.device
          # param.data = torch.from_numpy((tensor * mask[step]).astype(datatype)).to(weight_dev) # Keeps the learned gates params
          param.data = torch.from_numpy(mask[step].astype(datatype)).to(weight_dev) # Discards the learned gates param
          step += 1
  
  return model

def random_init(model, mask, gates):
  step = 0
  for m in model.modules():
    if isinstance(m, gates):
      for name, param in m.named_parameters():
        if "weight" in name:
          tensor = param.data.cpu().numpy()
          weight_dev = param.device
          # param.data = torch.from_numpy((tensor * mask[step]).astype(datatype)).to(weight_dev) # Keeps the learned gates params
          param.data = torch.from_numpy(mask[step].astype(datatype)).to(weight_dev) # Discards the learned gates param
          step += 1
  
  return model

def save_mask(mask, i):
  checkdir(f"{os.getcwd()}/saves/{modelType}/{datasetName}/")
  myfile = open(f"{os.getcwd()}/saves/{modelType}/{datasetName}/mask_{procedure}_{i}.txt", "wb")
  pickle.dump(mask, myfile)                     
  myfile.close()

def save_acc(acc, i):
  checkdir(f"{os.getcwd()}/saves/{modelType}/{datasetName}/")
  myfile = open(f"{os.getcwd()}/saves/{modelType}/{datasetName}/accuracy_{procedure}_{i}.txt", "wb")
  pickle.dump(save_acc, myfile)                     
  myfile.close()

def train(model, train_loader, optimizer, criterion, dense=False):
  prunable = 1e-6
  model.train()
  epoch_loss = 0
  for batch_idx, (imgs, targets) in enumerate(train_loader):
    optimizer.zero_grad()
    imgs, targets = imgs.to(device), targets.to(device)
    if dense:
      imgs = imgs.reshape(-1, 784)
    output = model(imgs)
    train_loss = criterion(output, targets)
    epoch_loss += train_loss.item() 
    train_loss.backward()
    optimizer.step()
  return epoch_loss

def test(model, test_loader, criterion, dense=False):
  model.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad():
    for data, target in test_loader:
      data, target = data.to(device), target.to(device)
      if dense:
        data = data.reshape(-1, 784)
      output = model(data)
      test_loss += F.nll_loss(output, target, reduction='sum').item()
      pred = output.data.max(1, keepdim=True)[1] 
      correct += pred.eq(target.data.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
  return accuracy

"""# Data Loading"""

transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

transform_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
])

trainset = torchvision.datasets.CIFAR10(root='./', train=True, download=True, transform=transform_train)
testset = torchvision.datasets.CIFAR10(root='./', train=False, download=True, transform=transform_test)

# trainset = torch.utils.data.Subset(trainset, [x for x in range(0, 1000)])
# testset = torch.utils.data.Subset(testset, [x for x in range(0, 200)])

trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)
testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)

"""# Network Setup"""

class Gates(nn.Module):
  def __init__(self, size):
    super().__init__()
    self.size = size
    self.weight = nn.Parameter(data=torch.ones(size),requires_grad=True)
  def forward(self, x):
    return x*torch.reshape(self.weight,(1,self.size,1,1))

cfg = {
    'test' : ['M', 'M', 'M', 'M', 2, 'M'],
    'smol' : [4, 'M', 8, 'M', 16, 'M', 32, 'M', 64, 'M'],
    'custom': [32, 'M', 64, 'M', 128, 'M', 256, 'M', 512, 'M'],
    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}

class VGG(nn.Module):
  def __init__(self, vgg_name):
    super(VGG, self).__init__()
    self.features = self._make_layers(cfg[vgg_name])
    self.classifier = nn.Linear(cfg[vgg_name][-2], 10)

  def forward(self, x):
    out = self.features(x)
    out = out.view(out.size(0), -1)
    out = self.classifier(out)
    return out

  def _make_layers(self, cfg):
    layers = []
    in_channels = 3
    for x in cfg:
      if x == 'M':
        layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
      else:
        layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),
                    Gates(x),
                    nn.BatchNorm2d(x),
                    nn.ReLU(inplace=True)]
        in_channels = x
    layers += [nn.AvgPool2d(kernel_size=1, stride=1)]
    return nn.Sequential(*layers)

"""# Initial Test"""

model = VGG("custom")
model = model.to(device)

summary(model, input_shape)

initial_state_dict = copy.deepcopy(model.state_dict())

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

bestacc = 0.0
best_accuracy = 0
comp = np.zeros(prune_iterations,float)
bestacc = np.zeros(prune_iterations,float)
step = 0
all_loss = np.zeros(epochs,float)
all_accuracy = np.zeros(epochs,float)
mask = make_mask(model, Gates)

for i in range(0, prune_iterations):
  if not i == 0:
    model = model_grad(model, Gates, True, False)
    print("The total params are: ", sum(p.numel() for p in model.parameters()))
    print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
    optimizer = torch.optim.Adam(model.parameters(), lr=10*lr)
    for k in range(gatepochs):
      loss = train(model, trainloader, optimizer, criterion)
    model, mask = prune(model, mask, prune_percent, Gates)
    model = lottery_init(model, mask, initial_state_dict, Gates)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  model = model_grad(model, Gates, False, True)
  print("The total params are: ", sum(p.numel() for p in model.parameters()))
  print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
  print(f"\n--- Pruning Level [{1}:{i}/{prune_iterations}]: ---")
  pbar = tqdm(range(epochs))
  for j in pbar:
    accuracy = test(model, testloader, criterion)
    if accuracy > best_accuracy:
      best_accuracy = accuracy
    loss = train(model, trainloader, optimizer, criterion)
    all_loss[j] = loss
    all_accuracy[j] = accuracy
    pbar.set_description(
      f'Train Epoch: {j}/{epochs} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')
  bestacc[i] = best_accuracy
  save_acc(bestacc[i], i)
  best_accuracy = 0

temp = np.arange(prune_iterations)
x = []
for item in temp:
  x.append(round((1-(1-(prune_percent/100))**item), 3)*100)
print(x)

print(bestacc)

plt.plot(x, bestacc, c="blue") 
plt.title(f"Test Accuracy vs Unpruned Weights Percentage") 
plt.xlabel("Sparsity (percentage)") 
plt.ylabel("test accuracy") 
plt.ylim(0,100)
plt.legend() 
plt.grid(color="gray") 
# checkdir(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/")
# plt.savefig(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/{procedure}_AccuracyVsWeights.png", dpi=1200)
plt.show() 
plt.close()

for item in mask:
  print(item)

for m in model.modules():
  if isinstance(m, Gates):
    for name, param in m.named_parameters():
      print(param)

"""# Baseline Comparison"""

# Hyperparameters
lr = 0.002
input_shape = (3, 32, 32)
epochs = 20
batch_size = 256
prune_iterations = 10
prune_percent = 20
num_classes = 10
modelType = "vgg16"
datasetName = "cifar10"
procedure = "UnstructuredLT"

"""# Baseline Helper Functions"""

# Utilities

def checkdir(directory):
  if not os.path.exists(directory):
    os.makedirs(directory)

def w_init(model):
  if isinstance(model, nn.Conv2d):
    init.xavier_normal_(model.weight.data)
    if model.bias is not None:
      init.normal_(model.bias.data)

def prune(model, mask, percent):
  step = 0
  for name, param in model.named_parameters():
    if 'weight' in name:
      tensor = param.data.cpu().numpy()
      alive = tensor[np.nonzero(tensor)]
      percentile_value = np.percentile(abs(alive), percent)

      weight_dev = param.device
      new_mask = np.where(abs(tensor) < percentile_value, 0, mask[step])
      
      param.data = torch.from_numpy(tensor * new_mask).to(weight_dev)
      mask[step] = new_mask
      step += 1
  return model, mask

def make_mask(model):
  step = 0
  for name, param in model.named_parameters(): 
    if 'weight' in name:
        step = step + 1
  mask = [None]*step 
  step = 0
  for name, param in model.named_parameters(): 
    if 'weight' in name:
      tensor = param.data.cpu().numpy()
      mask[step] = np.ones_like(tensor)
      step = step + 1
  
  return mask

def lottery_init(model, mask, state_dict):
  step = 0
  for name, param in model.named_parameters(): 
    if "weight" in name: 
      weight_dev = param.device
      param.data = torch.from_numpy(mask[step]*state_dict[name].cpu().numpy()).to(weight_dev)
      step = step + 1
    if "bias" in name:
      param.data = state_dict[name]
  return model

def random_init(model, mask):
  step = 0
  model.apply(w_init)
  for name, param in model.named_parameters():
    if 'weight' in name:
      weight_dev = param.device
      param.data = torch.from_numpy(param.data.cpu().numpy() * mask[step]).to(weight_dev)
      step = step + 1
  return model

# Training and Testing
def train(model, train_loader, optimizer, criterion, dense=False):
  prunable = 1e-6
  model.train()
  for batch_idx, (imgs, targets) in enumerate(train_loader):
    optimizer.zero_grad()
    imgs, targets = imgs.to(device), targets.to(device)
    if dense:
      imgs = imgs.reshape(-1, 784)
    output = model(imgs)
    train_loss = criterion(output, targets)
    train_loss.backward()

    for name, p in model.named_parameters():
      if 'weight' in name:
        tensor = p.data.cpu().numpy()
        grad_tensor = p.grad.data.cpu().numpy()
        grad_tensor = np.where(tensor < prunable, 0, grad_tensor)
        p.grad.data = torch.from_numpy(grad_tensor).to(device)
    optimizer.step()
  return train_loss.item()

def test(model, test_loader, criterion, dense=False):
  model.eval()
  test_loss = 0
  correct = 0
  with torch.no_grad():
    for data, target in test_loader:
      data, target = data.to(device), target.to(device)
      if dense:
        data = data.reshape(-1, 784)
      output = model(data)
      test_loss += F.nll_loss(output, target, reduction='sum').item()
      pred = output.data.max(1, keepdim=True)[1] 
      correct += pred.eq(target.data.view_as(pred)).sum().item()
    test_loss /= len(test_loader.dataset)
    accuracy = 100. * correct / len(test_loader.dataset)
  return accuracy

def save_mask(mask, i):
  checkdir(f"{os.getcwd()}/saves/{modelType}/{datasetName}/")
  myfile = open(f"{os.getcwd()}/saves/{modelType}/{datasetName}/mask_{procedure}_{i}.txt", "wb")
  pickle.dump(mask, myfile)                     
  myfile.close()

def save_acc(acc, i):
  checkdir(f"{os.getcwd()}/saves/{modelType}/{datasetName}/")
  myfile = open(f"{os.getcwd()}/saves/{modelType}/{datasetName}/accuracy_{procedure}_{i}.txt", "wb")
  pickle.dump(save_acc, myfile)                     
  myfile.close()

model = VGG("VGG11").to(device)

summary(model, input_shape)

model = model.apply(w_init)

initial_state_dict = copy.deepcopy(model.state_dict())

optimizer = torch.optim.Adam(model.parameters(), lr=lr)
criterion = nn.CrossEntropyLoss()
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

# for name, param in model.named_parameters():
#   print(name, param.size())
bestacc = 0.0
best_accuracy = 0
comp = np.zeros(prune_iterations,float)
bestacc = np.zeros(prune_iterations,float)
step = 0
all_loss = np.zeros(epochs,float)
all_accuracy = np.zeros(epochs,float)
mask = make_mask(model)

for i in range(0, prune_iterations):
  if not i == 0:
    model, mask = prune(model, mask, prune_percent)
    model = lottery_init(model, mask, initial_state_dict)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  print(f"\n--- Pruning Level [{1}:{i}/{prune_iterations}]: ---")
  pbar = tqdm(range(epochs))
  for j in pbar:
    accuracy = test(model, testloader, criterion)
    if accuracy > best_accuracy:
      best_accuracy = accuracy
    loss = train(model, trainloader, optimizer, criterion)
    all_loss[j] = loss
    all_accuracy[j] = accuracy
    pbar.set_description(
      f'Train Epoch: {j}/{epochs} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')
  bestacc[i] = best_accuracy
  save_acc(bestacc[i], i)
  best_accuracy = 0

temp = np.arange(prune_iterations)
x = []
for item in temp:
  x.append(round((1-(1-(prune_percent/100))**item), 3)*100)
print(x)

print(bestacc)

plt.plot(x, bestacc, c="blue") 
plt.title(f"Test Accuracy vs Unpruned Weights Percentage") 
plt.xlabel("Sparsity (percentage)") 
plt.ylabel("test accuracy") 
plt.ylim(0,100)
plt.legend() 
plt.grid(color="gray") 
# checkdir(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/")
# plt.savefig(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/{procedure}_AccuracyVsWeights.png", dpi=1200)
plt.show() 
plt.close()

"""# Second Test with original initialisation"""

model = VGG("VGG11")
model = model.to(device)

summary(model, input_shape)

initial_state_dict = copy.deepcopy(model.state_dict())

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

bestacc = 0.0
best_accuracy = 0
comp = np.zeros(prune_iterations,float)
bestacc = np.zeros(prune_iterations,float)
step = 0
all_loss = np.zeros(epochs,float)
all_accuracy = np.zeros(epochs,float)
mask = make_mask(model, Gates)

for i in range(0, prune_iterations):
  if not i == 0:
    model = model_grad(model, Gates, True, False)
    print("The total params are: ", sum(p.numel() for p in model.parameters()))
    print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
    optimizer = torch.optim.Adam(model.parameters(), lr=10*lr)
    for k in range(gatepochs):
      loss = train(model, trainloader, optimizer, criterion)
    model, mask = prune(model, mask, prune_percent, Gates)
    model = lottery_init(model, mask, initial_state_dict, Gates)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  model = model_grad(model, Gates, False, True)
  print("The total params are: ", sum(p.numel() for p in model.parameters()))
  print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
  print(f"\n--- Pruning Level [{1}:{i}/{prune_iterations}]: ---")
  pbar = tqdm(range(epochs))
  for j in pbar:
    accuracy = test(model, testloader, criterion)
    if accuracy > best_accuracy:
      best_accuracy = accuracy
    loss = train(model, trainloader, optimizer, criterion)
    all_loss[j] = loss
    all_accuracy[j] = accuracy
    pbar.set_description(
      f'Train Epoch: {j}/{epochs} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')
  bestacc[i] = best_accuracy
  save_acc(bestacc[i], i)
  best_accuracy = 0

temp = np.arange(prune_iterations)
x = []
for item in temp:
  x.append(round((1-(1-(prune_percent/100))**item), 3)*100)
print(x)

print(bestacc)

plt.plot(x, bestacc, c="blue") 
plt.title(f"Test Accuracy vs Unpruned Filters Percentage") 
plt.xlabel("Sparsity (percentage)") 
plt.ylabel("test accuracy") 
plt.ylim(0,100)
plt.legend() 
plt.grid(color="gray") 
# checkdir(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/")
# plt.savefig(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/{procedure}_AccuracyVsWeights.png", dpi=1200)
plt.show() 
plt.close()



"""# Third Test with random reinitialisation"""

model = VGG("VGG11")
model = model.to(device)

summary(model, input_shape)

initial_state_dict = copy.deepcopy(model.state_dict())

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)

bestacc = 0.0
best_accuracy = 0
comp = np.zeros(prune_iterations,float)
bestacc = np.zeros(prune_iterations,float)
step = 0
all_loss = np.zeros(epochs,float)
all_accuracy = np.zeros(epochs,float)
mask = make_mask(model, Gates)

for i in range(0, prune_iterations):
  if not i == 0:
    model = model_grad(model, Gates, True, False)
    print("The total params are: ", sum(p.numel() for p in model.parameters()))
    print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
    optimizer = torch.optim.Adam(model.parameters(), lr=10*lr)
    for k in range(gatepochs):
      loss = train(model, trainloader, optimizer, criterion)
    model, mask = prune(model, mask, prune_percent, Gates)
    model = VGG("VGG11")
    model = model.to(device)
    model = random_init(model, mask, Gates)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
  model = model_grad(model, Gates, False, True)
  print("The total params are: ", sum(p.numel() for p in model.parameters()))
  print("The trainable params are: ", sum(p.numel() for p in model.parameters() if p.requires_grad))
  print(f"\n--- Pruning Level [{1}:{i}/{prune_iterations}]: ---")
  pbar = tqdm(range(epochs))
  for j in pbar:
    accuracy = test(model, testloader, criterion)
    if accuracy > best_accuracy:
      best_accuracy = accuracy
    loss = train(model, trainloader, optimizer, criterion)
    all_loss[j] = loss
    all_accuracy[j] = accuracy
    pbar.set_description(
      f'Train Epoch: {j}/{epochs} Loss: {loss:.6f} Accuracy: {accuracy:.2f}% Best Accuracy: {best_accuracy:.2f}%')
  bestacc[i] = best_accuracy
  save_acc(bestacc[i], i)
  best_accuracy = 0

temp = np.arange(prune_iterations)
x = []
for item in temp:
  x.append(round((1-(1-(prune_percent/100))**item), 3)*100)
print(x)

print(bestacc)

plt.plot(x, bestacc, c="blue") 
plt.title(f"Test Accuracy vs Unpruned Filters Percentage") 
plt.xlabel("Sparsity (percentage)") 
plt.ylabel("test accuracy") 
plt.ylim(0,100)
plt.legend() 
plt.grid(color="gray") 
# checkdir(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/")
# plt.savefig(f"{os.getcwd()}/plots/{procedure}/{modelType}/{datasetName}/{procedure}_AccuracyVsWeights.png", dpi=1200)
plt.show() 
plt.close()

